{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM54xNKi3OiyUsH2g1W250h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takeshitayy/ipynb2markdown/blob/main/ipynb2markdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ipynb2markdown\n",
        "\n",
        "web上に公開されている Jupyter Notebook ファイル (ipynb) を markdown に変換する\n",
        "\n",
        "google colaboratory で実行する\n",
        "\n",
        "1. `url` に公開されている Notebook ファイル (ipynb) の URL を設定\n",
        "1. `ctrl + F9` ですべてを実行する\n",
        "1. マークダウン記法とプレビューが表示される\n",
        "\n"
      ],
      "metadata": {
        "id": "KT7D0XClPwI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#@title ipynb ファイルのURLを指定する\n",
        "url = \"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Classification_using_embeddings.ipynb\" #@param {type:\"string\"}\n",
        "\n",
        "if url.startswith(\"https://github.com\"):\n",
        "    # url が https://github.com の場合は、ipynb ファイルのurlに変換する\n",
        "    url = url.replace(\"https://github.com\", \"https://raw.githubusercontent.com\").replace(\"/blob\", \"\")\n",
        "\n",
        "ipynbFilename = os.path.basename(url)\n",
        "markdownFilename = ipynbFilename + '.md'\n",
        "print('URL: ' + url)\n",
        "print('ipynbFilename: ' + ipynbFilename)\n",
        "print('markdownFilename: '+ markdownFilename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T22JW7PQd7_",
        "outputId": "fe0c2572-acb4-40aa-b934-23a4094a2903"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Classification_using_embeddings.ipynb\n",
            "ipynbFilename: Classification_using_embeddings.ipynb\n",
            "markdownFilename: Classification_using_embeddings.ipynb.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ipynb ファイルをダウンロード"
      ],
      "metadata": {
        "id": "Ejq5vipgcbuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UjkgCX23cl4D"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # レスポンスのステータスコードが200（成功）の場合、ファイルを保存\n",
        "    with open(ipynbFilename, \"wb\") as file:\n",
        "        file.write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ipynb ファイルを マークダウンに変換"
      ],
      "metadata": {
        "id": "02O_vWmtdp2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nbconvert\n",
        "from nbconvert import MarkdownExporter\n",
        "\n",
        "# Markdownエクスポーターを作成\n",
        "exporter = MarkdownExporter()\n",
        "\n",
        " # .ipynbファイルを読み込み\n",
        "with open(ipynbFilename, \"r\", encoding=\"utf-8\") as ipynb_file:\n",
        "    notebook_content = ipynb_file.read()\n",
        "\n",
        "# .ipynbファイルを.mdファイルに変換\n",
        "markdownContent, _ = exporter.from_file(ipynbFilename)\n",
        "\n",
        "print(markdownContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOZYt7hOcr3v",
        "outputId": "e7f27497-0ef8-48c0-c9d5-cf1f5cb37135"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Classification using embeddings\n",
            "\n",
            "There are many ways to classify text. This notebook shares an example of text classification using embeddings. For many text classification tasks, we've seen fine-tuned models do better than embeddings. See an example of fine-tuned models for classification in [Fine-tuned_classification.ipynb](Fine-tuned_classification.ipynb). We also recommend having more examples than embedding dimensions, which we don't quite achieve here.\n",
            "\n",
            "In this text classification task, we predict the score of a food review (1 to 5) based on the embedding of the review's text. We split the dataset into a training and a testing set for all the following tasks, so we can realistically evaluate performance on unseen data. The dataset is created in the [Obtain_dataset Notebook](Obtain_dataset.ipynb).\n",
            "\n",
            "\n",
            "\n",
            "```python\n",
            "# imports\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import classification_report, accuracy_score\n",
            "\n",
            "# load data\n",
            "datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n",
            "\n",
            "df = pd.read_csv(datafile_path)\n",
            "df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert string to array\n",
            "\n",
            "# split data into train and test\n",
            "X_train, X_test, y_train, y_test = train_test_split(\n",
            "    list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n",
            ")\n",
            "\n",
            "# train random forest classifier\n",
            "clf = RandomForestClassifier(n_estimators=100)\n",
            "clf.fit(X_train, y_train)\n",
            "preds = clf.predict(X_test)\n",
            "probas = clf.predict_proba(X_test)\n",
            "\n",
            "report = classification_report(y_test, preds)\n",
            "print(report)\n",
            "\n",
            "```\n",
            "\n",
            "                  precision    recall  f1-score   support\n",
            "    \n",
            "               1       0.88      0.35      0.50        20\n",
            "               2       1.00      0.38      0.55         8\n",
            "               3       1.00      0.18      0.31        11\n",
            "               4       1.00      0.26      0.41        27\n",
            "               5       0.74      1.00      0.85       134\n",
            "    \n",
            "        accuracy                           0.77       200\n",
            "       macro avg       0.92      0.43      0.52       200\n",
            "    weighted avg       0.82      0.77      0.72       200\n",
            "    \n",
            "\n",
            "\n",
            "We can see that the model has learnt to distinguish between the categories decently. 5-star reviews show the best performance overall, and this is not too surprising, since they are the most common in the dataset.\n",
            "\n",
            "\n",
            "```python\n",
            "from openai.embeddings_utils import plot_multiclass_precision_recall\n",
            "\n",
            "plot_multiclass_precision_recall(probas, y_test, [1, 2, 3, 4, 5], clf)\n",
            "\n",
            "```\n",
            "\n",
            "    RandomForestClassifier() - Average precision score over all classes: 0.87\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "![png](output_3_1.png)\n",
            "    \n",
            "\n",
            "\n",
            "Unsurprisingly 5-star and 1-star reviews seem to be easier to predict. Perhaps with more data, the nuances between 2-4 stars could be better predicted, but there's also probably more subjectivity in how people use the inbetween scores.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## マークダウンの内容を表示"
      ],
      "metadata": {
        "id": "76YykfaEdgy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(markdownContent)"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/output_3_1.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1lRMhqi5dTSF",
        "outputId": "067ca923-fb24-4794-dbd1-e2d0f90d7b80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Classification using embeddings\n\nThere are many ways to classify text. This notebook shares an example of text classification using embeddings. For many text classification tasks, we've seen fine-tuned models do better than embeddings. See an example of fine-tuned models for classification in [Fine-tuned_classification.ipynb](Fine-tuned_classification.ipynb). We also recommend having more examples than embedding dimensions, which we don't quite achieve here.\n\nIn this text classification task, we predict the score of a food review (1 to 5) based on the embedding of the review's text. We split the dataset into a training and a testing set for all the following tasks, so we can realistically evaluate performance on unseen data. The dataset is created in the [Obtain_dataset Notebook](Obtain_dataset.ipynb).\n\n\n\n```python\n# imports\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# load data\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(eval).apply(np.array)  # convert string to array\n\n# split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n)\n\n# train random forest classifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\n\nreport = classification_report(y_test, preds)\nprint(report)\n\n```\n\n                  precision    recall  f1-score   support\n    \n               1       0.88      0.35      0.50        20\n               2       1.00      0.38      0.55         8\n               3       1.00      0.18      0.31        11\n               4       1.00      0.26      0.41        27\n               5       0.74      1.00      0.85       134\n    \n        accuracy                           0.77       200\n       macro avg       0.92      0.43      0.52       200\n    weighted avg       0.82      0.77      0.72       200\n    \n\n\nWe can see that the model has learnt to distinguish between the categories decently. 5-star reviews show the best performance overall, and this is not too surprising, since they are the most common in the dataset.\n\n\n```python\nfrom openai.embeddings_utils import plot_multiclass_precision_recall\n\nplot_multiclass_precision_recall(probas, y_test, [1, 2, 3, 4, 5], clf)\n\n```\n\n    RandomForestClassifier() - Average precision score over all classes: 0.87\n\n\n\n    \n![png](output_3_1.png)\n    \n\n\nUnsurprisingly 5-star and 1-star reviews seem to be easier to predict. Perhaps with more data, the nuances between 2-4 stars could be better predicted, but there's also probably more subjectivity in how people use the inbetween scores.\n"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}